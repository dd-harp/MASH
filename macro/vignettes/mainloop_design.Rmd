---
title: "Mainloop Design"
output:
  pdf_document:
    toc: true
    df_print: kable
    number_sections: yes
    dev: png
  html_notebook:
    df_print: paged
header-includes:
  - \usepackage{palatino}
---

```{r load_libraries, hide = TRUE}
library(igraph)
```

# Overview

The MASH simulation iterates over four steps, simulating human health, human movement,
mosquito lifecycle, and bloodmeals. These four steps could be implemented so separately
that they are in different languages, different computer programs, or computed remotely.
That separate implementation creates risk for how we write and run the loop that calls
each of those steps.

While the initial loop has four steps, there are useful simulations that may run two
modules together or combine a different and larger set of modules. We should be able
to write different main loops.

The main loop becomes useful when we can compare the effect of substituting one
location module for another or one bloodmeal module for another. We will be able
to substitute modules when they expect the same kinds of inputs and outputs. For instance,
a bloodmeal module needs to know how many food sources are in a location
and how many have parasites. We have to encode that information in a way that all
comparable bloodmeal modules can read. You can make a bloodmeal module that uses
different information, but it will have a different contract with the location module
providing that information. These contracts are our most expert product.

Meanwhile, there is a small matter of how to write this code in R.
We need a way to encode the relationships just discussed. We also
need quotidien affordances to read parameters and write results.
The most difficult question for the R implementation will be how we handle
running modules that can be unkind. They may use more memory and time. They
may raise warnings or exceptions. The degree to which we can treat modules as
components, in the software engineering sense, will guard the effort we put
into savvy contracts from the failings of lackadaisical coding.


# Explicit Calls with S3 Method Dispatch

We can express the main loop directly by writing code that passes data
from module to module, as described in the TWICE document. Ignore, for the moment,
how we create the modules and how we read results.

Here, a MASH step is a fourteen-day time step. A path is the results of a time
step, formatted in a way the next module needs to read them.
```{r, eval = FALSE}
location <- mash_step(location, health_path)
location_path <- location_path(location)
bloodmeal <- mash_step(bloodmeal, location_path, health_path)
bloodmeal_path <- infects_human_path(bloodmeal)
health <- mash_step(health, bloodmeal_path)
health_path <- human_disease_path(health)
```
If we define a location module as an S3 class, such as `single_location`,
we can write for it a method that returns that single location for each person.
```{r, eval = FALSE}
location_path.single_location <- function(location_module) rep(1, location_module$people_cnt)
```

We are using S3 just for its method dispatch. We're not using objects in a meaningful way
because each of these modules will be, we assume, implemented in their own
colorful manner, be it an R script or a Fortran program. There will be only a few
calls to these modules, one for initialization, one to step, and one or two to extract
results paths.



# Explicit Graph of Loop Order
We have to run a mainloop that has a bunch of modules that pass data to each other.
There could be four of them, but there could be four hundred, if we separate each patch
into a separate module.

Therefore, represent the execution graph of the modules as a graph and use
that graph to run them and pass data. None of these graphs will have any cycles.
Start with a graph that is linear.

```{r make_igraph}
module_graph <- graph_from_literal(bite1 -+ human1, human1 -+ move,
                                   bite2 -+ human2, human2 -+ move)
```

We need algorithms that

* determine where to start computing
* determine runnable modules so that they can run in parallel
* know when to get rid of data from memory, if possible.

We can order the modules such that running them in this order will always work.
```{r ordered}
names(topo_sort(module_graph))
```
We can get incident edges of one vertex.
```{r}
incident(module_graph, V(module_graph)["move"], mode = "in")
```
or get multiple incident edges
```{r}
incident_edges(module_graph, V(module_graph)["move"], mode = "in")
```
Which have no incident edges?
```{r}
incident_cnt <- lapply(incident_edges(module_graph, V(module_graph), mode = "in"), length)
t(as.data.frame(incident_cnt))
```
Can we use the graph data structure while we do our computation, to keep track
of what has been computed and what can be thrown away?
```{r}
vertex_attr(module_graph, "done") <- FALSE
vertex.attributes(module_graph)
```
Let's mark the bite1 as done. Then can we figure out that it's OK to run
human1?
```{r}
V(module_graph)["bite1"]$done <- TRUE
adjacent_vertices(module_graph, V(module_graph)["bite1"], mode = "out")
```
That returns the next vertex, but what if the next vertex depends on multiple
inputs? I'd like to query for all vertices whose input vertices are marked done.
```{r}
madj <- as_adjacency_matrix(module_graph)
```
That's the adjacency matrix. If we premultiply by the list of what is done,
changing it to 1 for done, 0 for not done, we get.
```{r}
inputs_available <- c(1, 0, 0, 0, 0) %*% madj
```
Wherever that value is less than the incidence for the vertex, it's ready to run.
```{r}
ready_to_run <- inputs_available == Matrix::Matrix(as.numeric(incident_cnt), nrow = 1)
ready_to_run
```

```{r}
require(igraph)
vertex_cnt <- 5
# a complete graph has n (n-1) / 2 edges.
edge_max <- 6
gg <- make_empty_graph(n = vertex_cnt)
for (add_idx in 1:5000) {
  candidate <- sample(1:vertex_cnt, 2)
  existing <- E(gg)[.from(candidate[1]) & .to(candidate[2])]
  if (length(existing) == 0) {
    added <- add.edges(gg, candidate)
    if (is.dag(added)) {
      gg <- added
      if (ecount(gg) == edge_max) break
    }
  }
}
```

What if we write the main loop and give ourselves a way to re-execute it
later?
```{r}
bite <- function() "bite1"
human <- function(bite) "human1"
move <- function(human) "move1"
example_main <- function(delayed) {
  b <- delayed(bite)()
  h <- delayed(human)(b)
  m <- delayed(move)(h)
  m
}
no_delay <- function(x) x
example_main(no_delay)
```
Try this again with a recording.
```{r}
delay <- function(f) {
  arguments <- names(as.list(f))
  arguments <- arguments[arguments != ""]
  function(...) {
    3
  }
}
```


# Observation

This simulation style will let us observe basic trajectories, no matter what
modules we run, because we insist that the API between modules exchange a particular
set of data structures. We can make a generic observer that always records
this information, or a subset of it.

That generic information doesn't include, for instance, detailed transitions for
each person. It's not relevant to other modules that a person is in a particular
compartment. Other modules will need to know probability of infection. If we
want to observe details about each person, the method that records these
details will need to collaborate with the specific human module.


# Parameters

We should make it easy to work with parameters. A user has some main needs.
I'll list them here, in order, with possible ways to meet those needs beneath.

1. Find possible parameters
   a. Look in the documentation.
   b. Function to query possible parameters.
2. Know default values and construct with them.
   a. Defaults could be set as default parameters in a constructor.
   b. Defaults would be in the documentation.
3. Save and load parameters.
   a. This could be an R rdata file.
   b. Save as HDF.
4. Change them during the simulation.
5. Know which ones can be changed during the simulation
6. Let the simulation see which ones were just changed by the user.
7. Check that parameters are reasonable before running a simulation
   a. Run a few steps of the main loop.
   b. Check parameters against known types.

Parameters are currently in a list.

There are levels of checks that parameters are reasonable. One sure way
to check is to run a step or two of the main loop. That's great for smaller
simulations, but I have in mind that we will run this on the cluster against
cluster-sized data, so it helps to have another kind of check, where you see
that the names of parameters are OK and that values are the correct type.

Getting parameter types wrong can be insidious. A wrong type can be converted
automatically into something you didn't expect, but which still runs.
